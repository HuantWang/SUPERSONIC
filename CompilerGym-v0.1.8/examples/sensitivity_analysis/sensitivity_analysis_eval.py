# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
"""Evaluate logs generated by sensitivity analysis.

Usage:

    $ bazel run -c opt //compiler_gym/bin:sensitivity_analysis_eval -- \
        --output_dir=/path/to/generated/logs \
        --analysis=actions_IrInstructionCountO3
"""
import sys
from concurrent.futures import Future, as_completed
from pathlib import Path
from typing import Iterable, NamedTuple

import numpy as np
from absl import app, flags

import compiler_gym.util.flags.output_dir  # noqa Flag definition.
from compiler_gym.util.tabulate import tabulate
from compiler_gym.util.timer import humanize_duration

flags.DEFINE_string(
    "analysis", None, "The name of the sensitivity analysis logs to read"
)

FLAGS = flags.FLAGS


class SensitivityAnalysisResult(NamedTuple):
    """The result of running a sensitivity analysis."""

    # The name of the thing being analyzed (e.g. an action or a benchmark).
    name: str
    # A list of runtimes, one per observation.
    runtimes: np.ndarray
    # A list of reward deltas, one per observation.
    rewards: np.ndarray


def run_sensitivity_analysis(
    analysis_futures: Iterable[Future],
    rewards_path: Path,
    runtimes_path: Path,
) -> None:
    """Run the provided sensitivity analyses to completion and log the results.

    :param analysis_futures: A sequence of future analysis results. The future
        should return a SensitityAnalysisResult.
    :param rewards_path: The path of the CSV file to write rewards to.
    :param runtimes_path: The path of the CSV file to write runtimes to.
    """
    rewards_path.parent.mkdir(parents=True, exist_ok=True)
    runtimes_path.parent.mkdir(parents=True, exist_ok=True)
    print(f"Writing rewards to {rewards_path}", file=sys.stderr)
    print(f"Writing runtimes to {runtimes_path}", file=sys.stderr)

    print("Waiting for first result ... ", end="", flush=True, file=sys.stderr)
    with open(str(rewards_path), "w") as rewards_f, open(
        str(runtimes_path), "w"
    ) as runtimes_f:
        for i, future in enumerate(as_completed(analysis_futures), start=1):
            result: SensitivityAnalysisResult = future.result()
            print(
                f"\r\033[KCompleted {i} of {len(analysis_futures)} analyses. "
                f"Latest: {result.name}, "
                f"avg_delta={result.rewards.mean():.5%}, "
                f"avg_runtime={humanize_duration(result.runtimes.mean())} ... ",
                end="",
                flush=True,
                file=sys.stderr,
            )
            print(
                result.name,
                ",".join(str(a) for a in result.rewards),
                sep=",",
                flush=True,
                file=rewards_f,
            )
            print(
                result.name,
                ",".join(str(a) for a in result.runtimes),
                sep=",",
                flush=True,
                file=runtimes_f,
            )
    print(flush=True, file=sys.stderr)

    return run_sensitivity_analysis_eval(rewards_path, runtimes_path)


def run_sensitivity_analysis_eval(rewards_path: Path, runtimes_path: Path) -> None:
    """Print a summary of sensitivity analysis logs."""
    with open(str(rewards_path)) as f:
        rewards_in = f.read().rstrip().split("\n")
    with open(str(runtimes_path)) as f:
        runtimes_in = f.read().rstrip().split("\n")

    rows = []
    for rewards_row, runtimes_row in zip(rewards_in, runtimes_in):
        name, *rewards = rewards_row.split(",")
        _, *runtimes = runtimes_row.split(",")
        if rewards == [""]:
            rows.append((name, "-", "-", "-", "-", "-"))
            continue
        rewards = np.array([float(v) for v in rewards])
        runtimes = np.array([float(v) for v in runtimes])
        rows.append(
            (
                name,
                humanize_duration(runtimes.mean()),
                f"{rewards.mean():.5%}",
                f"{np.median(rewards):.5%}",
                f"{rewards.max():.5%}",
                f"{rewards.std():.5%}",
            )
        )

    print(
        tabulate(
            sorted(rows),
            headers=(
                "Name",
                "Time (avg)",
                "Δ (avg)",
                "Δ (median)",
                "Δ (max)",
                "Δ (std.)",
            ),
        )
    )


def main(argv):
    """Main entry point."""
    argv = FLAGS(argv)
    if len(argv) != 1:
        raise app.UsageError(f"Unknown command line arguments: {argv[1:]}")

    assert FLAGS.output_dir, "Required argument --logs_path not set"
    assert FLAGS.analysis, "Required argument --analysis not set"

    output_dir = Path(FLAGS.output_dir)
    rewards_path = output_dir / f"{FLAGS.analysis}.rewards.csv"
    runtimes_path = output_dir / f"{FLAGS.analysis}.runtimes.csv"

    run_sensitivity_analysis_eval(
        rewards_path=rewards_path, runtimes_path=runtimes_path
    )


if __name__ == "__main__":
    app.run(main)
