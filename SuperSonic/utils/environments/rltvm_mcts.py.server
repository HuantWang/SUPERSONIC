# https://www.cnblogs.com/pinard/p/10609228.html   
# 这里是上层mcts
# 在这里进行交互, 交互参数有四个, action_remote, reward_remote, state_remote, maxLen
# 将交互得到的数据传递给cg环境
# cg环境是在这里的init创建的环境


import grpc
import rl2_pb2
import rl2_pb2_grpc
# 因为 RPC 应该长时间运行，考虑到性能，还需要用到并发的库。
import time # 设置系统延时,
from concurrent import futures
_ONE_DAY_IN_SECONDS = 60 * 60 * 24 # 设置服务器运行的默认时间长度
#add by zc
from multiprocessing import  Lock
import threading
import numpy as np
#同步锁
lock = threading.Lock()
lock_s = threading.Lock()
# 设定全局变量
state_remote = """
[[('tile_ic', [-1, 2]),
    ('tile_oc', [-1, 32]), 
    ('tile_ow', [-1, 2]), 
    ('unroll_kw', True)],
    None,151]"""
action_remote = 0
reward_remote = 0.0
maxLen_start = 2000
init_space = np.array([1 for _ in range(maxLen_start)])
maxLen = 2000



class MsgServicer(rl2_pb2_grpc.MsgServiceServicer):
    def GetMsg(self, request, context):
        global state_remote 
        global action_remote
        global reward_remote 
        global maxLen 

        # print("获得的数据是: ", request.state, "\n", "reward: ", request.reward)
        print(f"获得的数据有reward={reward_remote}, maxLen={maxLen}, 发送出去的action={action_remote}")
        # lock_s.acquire()

        state_remote = request.state
        reward_remote = request.reward
        if maxLen != request.maxLen:
            print("这里修改了malen")
            maxLen = request.maxLen
            for _ in range(maxLen):
                init_space[_] = 1 # 这里根据传回来的最大action范围进行限定
            for _ in range(maxLen, maxLen_start):
                init_space[_] = 0 # 这里根据传回来的最大action范围进行限定


        # print("init_space:::::::::::::::", init_space)
        print("naxlen:::::::::::::::", maxLen)
        if lock.locked():
            lock.release() # 只有这里有请求了, 才会让step继续执行下去
            print("lock release")
        return rl2_pb2.MsgResponse(action=action_remote) # 返回的参数赋值

from copy import deepcopy
from gensim.models.doc2vec import Doc2Vec,LabeledSentence
import gym
import compiler_gym
from gym.spaces import Discrete, Dict, Box
from gym import spaces
import logging
import random

DEBUG = False
#end by zc grpc
logger = logging.getLogger(__name__)

#NeuroVectorizer RL Environment
class mcts(gym.Env):
    def init_from_env_config(self, env_config):
        self.inference_mode = env_config.get("inference_mode", False)
        if self.inference_mode:
            self.improvements = []

    def __init__(self, env_config):
        self.init_from_env_config(env_config)
        self.server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))  # 设置最大连接数
        rl2_pb2_grpc.add_MsgServiceServicer_to_server(MsgServicer(), self.server)  #
        self.server.add_insecure_port('[::]:50053')  # 设置所有IP地址都可以访问, 端口号为50051
        self.server.start()  # 启动服务
        self.env = gym.make("Tvm-v0",
                            observation=env_config.get("observation"),
                            action=env_config.get("action"),
                            reward=env_config.get("reward"),)

        self.interleave_action_meaning = [_ for _ in range(maxLen_start )] # TODO: 根据需要更改action的空间
        self.action_space = spaces.Discrete(len(self.interleave_action_meaning)) #action_len
        self.observation_space = Dict({
            "obs": self.env.observation_space, # 调用的是cg的环境
            "action_mask": Box(low=0, high=1, shape=(self.action_space.n,))
        })
        self.running_reward = 0
        # self.server.stop() # 关闭服务

    def reset(self):
        self.running_reward = 0
        return {"obs": self.env.reset(), "action_mask":  init_space}
        #self.action_space.n/2
        # TODO：return这里，返回初始的obs和action_mask就可以

    def step(self,action):
        print("给客户端上锁, 等到grpc请求完成在进行")
        lock.acquire() # 这里判断是否已经有锁, 没有的话就上锁, 然后向下执行, 否则等待解锁
        global action_remote
        action_remote = action
        obs, rew, done, info = self.env.step(action_remote, state_remote, reward_remote)# TODO: state_remote, reward_remote
        self.running_reward += rew
        score = self.running_reward if done else 0
        init_space[action] = 0;
        print("运行完了, 进入下一轮")
        if lock_s.locked(): 
           lock_s.release()
           print("lock_s release")
        return {"obs": obs, "action_mask": init_space}, score, done,info #下一步的obs，reward等信息


    def set_state(self, state): #设置模拟的内部状态。state：要在环境中设置的目标状态。
        self.running_reward = state[1]
        self.env = deepcopy(state[0])
        obs = np.array(list(self.env.unwrapped.state))
        return {"obs": obs, "action_mask": init_space}

    def get_state(self):
        return deepcopy(self.env), self.running_reward
